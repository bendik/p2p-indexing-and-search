# A Simple Database

Well done for making it this far! At this stage, you've worked through the meat of what makes Hypercore-based data structures tick. Let's take a step back and do a quick recap:

We've introduced the basic principles behind Hypercore, and figured out how to use embedded indexes to build efficient data structures on top of append-only logs. These indexes add some size overhead, but they dramatically decrease the number of network lookups required for lookups. Since Hypercore allows us to "sparsely sync" the blocks we need on-demand, reducing the number of network requests is the name of the game -- that's what leads to great performance.

So far, we've been working with time-ordered data, and we saw how that ordering was essential for building the embedded index. Time-ordering is a pretty strict requirement, though. In this exercise, we'll relax that a bit.

But first for a bit of terminology: what you built in the previous exercises is a limited form of a "key/value store", or "kv-store" for short. Time is the key, and your process statistics are the values. There are two basic operations here: `put` a timestamp/stats pair, and `get` the stats for a given timestamp.

We really want you to get to the next exercises, where you get to explore a database we've built called Hyperbee, so the exercises here are optional and hidden. If you'd like to apply the algorithms you wrote for the previous exercises to kv-pairs (Webster's English Dictionary), expand the tab below.

<details>
  <summary><b>Optional Exercise</b></summary>

Enough with time. Let's extend the embedded indexing system from the previous exercises to work with arbitrary kv-pairs.

First, make a copy of your code from Exercises 7/8.

Next install the NPM module `websters-english-dictionary`. It exports a function called `kvPairs()` which you can use to get an array of Objects of the form:
```js
[
  { key: 'some-word', value: 'The word\'s definition' },
  ...
]
```

Because your indexing system assumes the entries are appended in sorted-order, we're going to have to pre-sort the list of kv-pairs before appending them to our Hypercore. You can do this with the following code snippet:
```js
// Will sort the kv-pairs in-place.
const { kvPairs } = require('websters-english-dictionary')

function sortedDictionaryPairs () {
  const pairs = kvPairs()
  pairs.sort((a, b) => {
    if (a.key < b.key) return -1
    if (a.key > b.key) return 1
    return 0
  })
  return pairs
}
```

Now you'll want to modify the code from Exercise 7 to work with kv-pairs (with String keys) instead of timestamps/stats. Once you've finished that, try inserting a subset of the dictionary (the examples below assume you inserted the first 10k words), and find the closest definitions to the following keys:
1. `aa' (should be 'aam')
2. 'bi' (should be 'bi')
3. 'buf' (should be 'bipectinated')
3. 'aile' (should be 'aileron')

*Note: You shouldn't have to make very many code changes to support kv-pairs! It boils down to replacing references to `timestamp` with references to `key`.*

Try `console.log`ing the indexes as you insert them, to get a better sense of how it's working!
</details>

## Stuck?

Take a look at the [solution here](/solutions/09/index.js).

## Takeaways

We're starting the see that the indexing technique we've been using is more flexible than it initially appeared. As long as your input dataset is appended to the log in sorted order, the algorithms from before will "just work" with arbitrary kv-pairs. But what if the input dataset isn't sorted, as is usually the case?

Most databases use a tried-and-true data structure called a B-Tree to solve this problem (take a look at the [Wikipedia page](https://en.wikipedia.org/wiki/B-tree) if you're not familiar with these), but constructing a B-Tree on top of an append-only log is tricky. Fortunately, we've extended the embedded indexing technique you used in the previous exercises to build an append-only B-Tree called [Hyperbee](https://github.com/mafintosh/hyperbee)!

Without further ado, it's time to [dive into Hyperbee in Problem 10](10.md).

[Continue to problem 10](10.md)
