## Querying with Hyperbee -- Key Encodings
* Let's try using the secondary index from the previous exercise to answer the question "Which definitions are between 0 and 10 characters long?" We'd express that query as:
```
db.createReadStream({ gt: '0/', lt: '20/' })
```
* When you run this query, you'll notice that the results are incorrect! Mixed in there are entries with length 100, 101...none of which belong.
* The problem is that keys are sorted according to their byte representations, and we're representing numeric lengths as strings. To illustrate the problem, notice that `'100' < '20' === true`. To make sure that our numeric keys sort correctly, we need to encode them appropriately.
* A simple way to fix this is to pad the keys. Assuming we know what the largest length in our dictionary is, we can zero-pad all the lengths, so that each key has the same length. This works because `'100' < '020' === false`.
* Unfortunately, it's rarely the case that we know the largest key value, so we need to be more clever. Luckily, there's an NPM module called `lexicographic-integer` that solves this problem for us.

#### Exercises
* Recreate the dictionary database from the previous exercise, but modify your secondary index to encode the keys as follows:
```
const lexint = require('lexicographic-integer')
function createIndexKey (word, definition) {
  return `${lexint.pack(definition.length, 'hex')}/${word}`
}
```
* Test your new secondary index on the following query. As a bonus, try running this query on the previous secondary index and see how it fails:
    * "All definitions between lengths 0 and 10" (there should be N)

#### Segue
* We've now explored how to use key encodings to create secondary indexes. With these additional indexes, we can satisfy sophisticated range queries efficiently.
* Using these patterns, let's start indexing and querying some large, real-world datasets.

[Continue to problem 14](14.md)
