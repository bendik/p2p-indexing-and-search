## Query a Big, Remote Hypercore

Let's see how well your algorithm works on a really big dataset! In the previous example, we were syncing between two peers on the same machine for testing purposes. That was useful for testing our the algorithms, but it obscured how the algorithms perform in real-world scenarios.

We've been collecting a bunch of process stats on our server. In the next exercise, we'll run the algorithms we built in Exercise 5 against this real-world dataset. 

*Note: Once people in the workshop start playing with this core, subsequent participants will start swarming with them. This is a good thing, but it does mean that if you're late to the party, you might see lower latencies all-around in this exercise (by connecting to closer peers).*

## New Methods
* Hypercore's [`download`](https://github.com/hypercore-protocol/hypercore#const-id--feeddownloadrange-callback): Request that a range of blocks be downloaded from remote peers. Defaults to requesting the entire core.

## Exercise

Here's the key to our big Hypercore, sitting somewhere in Germany: 
```
309ba1736cc5af2940fc9fb03d256547b78ca9972ce4398648a7d7782d31ad4b
```

Swap the old key you were using with this new one, and see how your algorithms fare against a big, remote Hypercore.

You might want to modify your solution to let you toggle between naive mode and binary search mode. Re-run it a few times and compare the results. The binary search algorithm should arrive at an answer having downloaded ~11 blocks, whereas the naive algorithm will need thousands.

You might find that the naive approach doesn't work at all here. That's because each `core.get` requires a round-trip to a remote peer -- now that we're talking to actual remote peers. A bunch of round-trips, one after the other, adds up to a lot of latency in total.

If you're curious, take a look at Hypercore's `download` API. Using `download`, we can explicitly tell Hypercore to start fetching blocks from the network, and this can be used to optimistically "prefetch".

Since the naive algorithm requires syncing a large portion of the core anyway, let's try downloading the whole thing in parallel. Try running the following, just before executing the naive algorithm:
```js
core.download() // No await here!
```

The prefetching should make the whole thing go a lot faster, because internally downloads are being satisfied in parallel, instead of serially (in response to a `get`). This does, however, come at the cost of over-downloading, because `core.download()` signals that the entire core should be downloaded.

## Takeaways
Using the naive approach, the reader needs to download the entire log, even though they're only interested in one block. It works, but it's slow and bandwidth-intensive, especially on large datasets. It might even be prohibitively slow, because each individual `get` triggers a network roundtrip to other peers.

*Sidenote: On the plus side, after the query the reader has downloaded a complete replica of the log, which means the reader will help keep the complete Hypercore available to the network if the writer goes offline. This is an important trade-off in our ecosystem: the more data each reader syncs, the better the overall network health is, but it comes at the cost of having to download more data.*

But we can do a lot better by taking advantage of the fact that the data is time-ordered, or "indexed by time". The dramatic performance improvement using the binary search algorithm highlights this. But we can do better still... 

[In Problem 7, we'll learn about embedded indexes](07.md)